{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIw6Al-9IlC"
      },
      "source": [
        "### Preparation"
      ],
      "id": "qMIw6Al-9IlC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy5s8y5eO6U_"
      },
      "outputs": [],
      "source": [
        "!pip install facenet_pytorch\n",
        "!pip install einops"
      ],
      "id": "Iy5s8y5eO6U_"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHjiWTQ7UM24",
        "outputId": "f76b63a6-6421-4dca-ab62-66fd2784b9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "root_dir = \"/content/drive/MyDrive\" # Set appropriate directory\n",
        "os.chdir(root_dir)"
      ],
      "id": "lHjiWTQ7UM24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV8PKmXwUPZo"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/data.zip -d /content"
      ],
      "id": "eV8PKmXwUPZo"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a0e5ed9d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "import torchvision\n",
        "from random import choice\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "# from torch.nn import Parameter"
      ],
      "id": "a0e5ed9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQi42At59ST5"
      },
      "source": [
        "### Config"
      ],
      "id": "NQi42At59ST5"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6csNcX0w9TfN"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_file_path = \"./data/train-relationships/train_relationships.csv\"\n",
        "    train_folders_path = \"/content/data/train/\"\n",
        "    val_famillies = \"F09\"\n",
        "    test_relationship_file = \"/content/data/submissions/sample_submission.csv\"\n",
        "    batch_size = 64\n",
        "    number_of_epochs = 100\n",
        "\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    MIN_NUM_PATCHES = 16\n",
        "    pretrained_vits = '/content/drive/MyDrive/face_transformer/Backbone_VITs_Epoch_2_Batch_12000_Time_2021-03-17-04-05_checkpoint.pth'\n",
        "    pretrained_vit = '/content/drive/MyDrive/face_transformer/Backbone_VIT_Epoch_2_Batch_20000_Time_2021-01-12-16-48_checkpoint.pth'"
      ],
      "id": "6csNcX0w9TfN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djujZnaA9PCk"
      },
      "source": [
        "### Dataset"
      ],
      "id": "djujZnaA9PCk"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9827674f"
      },
      "outputs": [],
      "source": [
        "class KinDataset(Dataset):\n",
        "    def __init__(self, relations, person_to_images_map, transform1, transform2):\n",
        "        self.relations = relations\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "        self.person_to_images_map = person_to_images_map\n",
        "        self.ppl = list(person_to_images_map.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.relations)*2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if (idx%2==0): #Positive samples\n",
        "            p1, p2 = self.relations[idx//2]\n",
        "            label = 1\n",
        "        else:          #Negative samples\n",
        "            while True:\n",
        "                p1 = choice(self.ppl)\n",
        "                p2 = choice(self.ppl)\n",
        "                if p1 != p2 and (p1, p2) not in self.relations and (p2, p1) not in self.relations:\n",
        "                    break\n",
        "            label = 0\n",
        "\n",
        "        path1, path2 = choice(self.person_to_images_map[p1]), choice(self.person_to_images_map[p2])\n",
        "        im1, im2 = Image.open(path1), Image.open(path2)\n",
        "\n",
        "        img1, img2 = self.transform1(im1), self.transform1(im2)\n",
        "        img3, img4 = self.transform2(im1), self.transform2(im2)\n",
        "\n",
        "        return img1, img2, img3, img4, label"
      ],
      "id": "9827674f"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeee53d7",
        "outputId": "5e8c6e75-347d-4805-a676-0f358e0e92da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare data...\n"
          ]
        }
      ],
      "source": [
        "print(\"Prepare data...\")\n",
        "all_images = glob(Config.train_folders_path + \"*/*/*.jpg\")\n",
        "\n",
        "train_images = [x for x in all_images if Config.val_famillies not in x]\n",
        "val_images = [x for x in all_images if Config.val_famillies in x]\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
        "\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "relationships = pd.read_csv(Config.train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
        "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
        "\n",
        "train_relations = [x for x in relationships if Config.val_famillies not in x[0]]\n",
        "val_relations  = [x for x in relationships if Config.val_famillies in x[0]]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(130),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0., 0., 0.],\n",
        "                         std=[1/255., 1/255., 1/255.])\n",
        "])\n",
        "train_transform2 = transforms.Compose([\n",
        "    transforms.Resize(160),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(130),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0., 0., 0.],\n",
        "                         std=[1/255., 1/255., 1/255.])\n",
        "])\n",
        "val_transform2 = transforms.Compose([\n",
        "    transforms.Resize(160),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "trainset = KinDataset(train_relations, train_person_to_images_map, train_transform, train_transform2)\n",
        "valset = KinDataset(val_relations, val_person_to_images_map, val_transform, val_transform2)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=Config.batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=Config.batch_size, shuffle=False)"
      ],
      "id": "eeee53d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad239ab6"
      },
      "source": [
        "### Model"
      ],
      "id": "ad239ab6"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5OT1FVv-BCdD"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "        #embed()\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, mask_value)\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n",
        "            ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask = mask)\n",
        "            #embed()\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT_face(nn.Module):\n",
        "    def __init__(self, *, loss_type, GPU_ID, num_class, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.loss_type = loss_type\n",
        "        self.GPU_ID = GPU_ID\n",
        "        if self.loss_type == 'None':\n",
        "            print(\"no loss for vit_face\")\n",
        "        else:\n",
        "            self.loss = CosFace(in_features=dim, out_features=num_class, device_id=self.GPU_ID)\n",
        "\n",
        "    def forward(self, img, label= None , mask = None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        last_hidden_state = x.detach()\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        emb = self.mlp_head(x)\n",
        "        if label is not None:\n",
        "            x = self.loss(emb, label)\n",
        "            return x, emb\n",
        "        else:\n",
        "            return emb, last_hidden_state\n",
        "\n",
        "class ViTs_face(nn.Module):\n",
        "    def __init__(self, *, loss_type, GPU_ID, num_class, image_size, patch_size, ac_patch_size,\n",
        "                         pad, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * ac_patch_size ** 2\n",
        "        assert num_patches > Config.MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.soft_split = nn.Unfold(kernel_size=(ac_patch_size, ac_patch_size), stride=(self.patch_size, self.patch_size), padding=(pad, pad))\n",
        "\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.loss_type = loss_type\n",
        "        self.GPU_ID = GPU_ID\n",
        "        if self.loss_type == 'None':\n",
        "            print(\"no loss for vit_face\")\n",
        "\n",
        "    def forward(self, img, label= None , mask = None, return_lhs=False):\n",
        "        p = self.patch_size\n",
        "        x = self.soft_split(img)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.patch_to_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        last_hidden_state = x.detach()\n",
        "        # print('transformer_out', x.shape)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        emb = self.mlp_head(x)\n",
        "        if label is not None:\n",
        "            x = self.loss(emb, label)\n",
        "            return x, emb\n",
        "        elif return_lhs:\n",
        "            return emb, last_hidden_state\n",
        "        else:\n",
        "            return emb\n",
        "\n",
        "def build_encoder(name):\n",
        "    if name == 'vits':\n",
        "        model = ViTs_face(\n",
        "            loss_type=None,\n",
        "            GPU_ID=Config.device,\n",
        "            num_class=93431,\n",
        "            image_size=112,\n",
        "            patch_size=8,\n",
        "            ac_patch_size=12,\n",
        "            pad=4,\n",
        "            dim=512,\n",
        "            depth=20,\n",
        "            heads=8,\n",
        "            mlp_dim=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "        model.load_state_dict(torch.load(Config.pretrained_vits, map_location=Config.device), strict=False)\n",
        "    else:\n",
        "        model = ViT_face(\n",
        "            image_size=112,\n",
        "            patch_size=8,\n",
        "            loss_type=None,\n",
        "            GPU_ID=Config.device,\n",
        "            num_class=93431,\n",
        "            dim=512,\n",
        "            depth=20,\n",
        "            heads=8,\n",
        "            mlp_dim=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "        model.load_state_dict(torch.load(Config.pretrained_vit, map_location=Config.device), strict=False)\n",
        "    return model"
      ],
      "id": "5OT1FVv-BCdD"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rJK5GvKH-tBt"
      },
      "outputs": [],
      "source": [
        "class AttentionPool2d(nn.Module):\n",
        "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
        "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
        "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
        "        x, _ = F.multi_head_attention_forward(\n",
        "            query=x[:1], key=x, value=x,\n",
        "            embed_dim_to_check=x.shape[-1],\n",
        "            num_heads=self.num_heads,\n",
        "            q_proj_weight=self.q_proj.weight,\n",
        "            k_proj_weight=self.k_proj.weight,\n",
        "            v_proj_weight=self.v_proj.weight,\n",
        "            in_proj_weight=None,\n",
        "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
        "            bias_k=None,\n",
        "            bias_v=None,\n",
        "            add_zero_attn=False,\n",
        "            dropout_p=0,\n",
        "            out_proj_weight=self.c_proj.weight,\n",
        "            out_proj_bias=self.c_proj.bias,\n",
        "            use_separate_proj_weight=True,\n",
        "            training=self.training,\n",
        "            need_weights=False\n",
        "        )\n",
        "        return x.squeeze(0)"
      ],
      "id": "rJK5GvKH-tBt"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c5d3AKUDvI-x"
      },
      "outputs": [],
      "source": [
        "# resnet = InceptionResnetV1(pretrained='vggface2')\n",
        "# cnn = torch.nn.Sequential(*(list(resnet.children())[:-5]))\n",
        "# cnn(torch.randn(2, 3, 160, 160)).shape"
      ],
      "id": "c5d3AKUDvI-x"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bk64amZK_sZQ"
      },
      "outputs": [],
      "source": [
        "class SiameseNet(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = build_encoder(name)\n",
        "        resnet = InceptionResnetV1(pretrained='vggface2')\n",
        "        self.cnn = torch.nn.Sequential(*(list(resnet.children())[:-5]))\n",
        "\n",
        "        self.attn_pool = AttentionPool2d(3, 1792, 1792//64, 512)\n",
        "\n",
        "        self.embed_size=512\n",
        "        self.last1 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size*2, 512),\n",
        "            nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1)\n",
        "        )\n",
        "        self.last2 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size*2, 512),\n",
        "            nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1)\n",
        "        )\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(1))\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size*3, self.embed_size*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.01),\n",
        "            nn.Linear(self.embed_size*4, self.embed_size*1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.01),\n",
        "            nn.Linear(self.embed_size*1, self.embed_size//4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.last1 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size//4+1,self.embed_size//16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(self.embed_size//16, 1),\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size*3, self.embed_size*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(self.embed_size*4, self.embed_size*1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(self.embed_size*1, self.embed_size//4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.last2 = nn.Sequential(\n",
        "            nn.Linear(self.embed_size//4+1,self.embed_size//16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(self.embed_size//16, 1),\n",
        "        )\n",
        "\n",
        "        flag = False # train last MHA in vit\n",
        "        for name, layer in self.encoder.named_parameters():\n",
        "            if '19' in name:\n",
        "                flag = True\n",
        "            if flag:\n",
        "                layer.requires_grad_(True)\n",
        "            else:\n",
        "                layer.requires_grad_(False)\n",
        "\n",
        "    def forward(self, input1, input2, input3, input4):\n",
        "\n",
        "        emb1 = self.encoder(input1)\n",
        "        emb2 = self.encoder(input2)\n",
        "        emb3 = self.attn_pool(self.cnn(input3))\n",
        "        emb4 = self.attn_pool(self.cnn(input4))\n",
        "\n",
        "        diff_v = emb1 - emb2\n",
        "        ssq_v = torch.pow(emb1,2) + torch.pow(emb2,2)\n",
        "        mul_v = emb1 * emb2\n",
        "        x1 = torch.cat([diff_v, ssq_v, mul_v],dim=-1)\n",
        "        x1 = self.fc1(x1)\n",
        "        cos_dis1 = 1-F.cosine_similarity(emb1, emb2, dim=-1)\n",
        "        x1 = torch.cat([x1, cos_dis1.unsqueeze(1)], dim=-1)\n",
        "        res1 = self.last1(x1)\n",
        "\n",
        "        diff_c = emb3 - emb4\n",
        "        ssq_c = torch.pow(emb3,2) + torch.pow(emb4,2)\n",
        "        mul_c = emb3 * emb4\n",
        "        x2 = torch.cat([diff_c, ssq_c, mul_c],dim=-1)\n",
        "        x2 = self.fc2(x2)\n",
        "        cos_dis2 = 1-F.cosine_similarity(emb3, emb4, dim=-1)\n",
        "        x2 = torch.cat([x2, cos_dis2.unsqueeze(1)], dim=-1)\n",
        "        res2 = self.last2(x2)\n",
        "\n",
        "        result = self.weight*res1 + (1-self.weight)*res2\n",
        "        return torch.sigmoid(result)"
      ],
      "id": "bk64amZK_sZQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU_IT59I-vBH"
      },
      "source": [
        "### Train"
      ],
      "id": "UU_IT59I-vBH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a09b76d4"
      },
      "outputs": [],
      "source": [
        "def train(net, criterion, optimizer):\n",
        "    net.train()\n",
        "    train_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img1, img2, img3, img4, label = batch\n",
        "        img1, img2, img3, img4, label = img1.to(Config.device), img2.to(Config.device), img3.to(Config.device), img4.to(Config.device), label.float().view(-1,1).to(Config.device)\n",
        "        output = net(img1, img2, img3, img4)\n",
        "        preds = output>0.5\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == (label>0.5))\n",
        "\n",
        "    train_loss /= len(trainset)\n",
        "    running_corrects = running_corrects.item()/len(trainset)\n",
        "    print('[{}], \\ttrain loss: {:.5}\\tacc: {:.5}'.format(epoch+1, train_loss, running_corrects))\n",
        "    return train_loss, running_corrects"
      ],
      "id": "a09b76d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bffb2919"
      },
      "outputs": [],
      "source": [
        "def validate(net, criterion, optimizer):\n",
        "    net.eval()\n",
        "    val_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for batch in valloader:\n",
        "        img1, img2, img3, img4, label = batch\n",
        "        img1, img2, img3, img4, label = img1.to(Config.device), img2.to(Config.device), img3.to(Config.device), img4.to(Config.device), label.float().view(-1,1).to(Config.device)\n",
        "        with torch.no_grad():\n",
        "            output = net(img1, img2, img3, img4)\n",
        "            preds = output>0.5\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == (label>0.5))\n",
        "\n",
        "    val_loss /= len(valset)\n",
        "    running_corrects = running_corrects.item()/len(valset)\n",
        "    print('[{}], \\tval loss: {:.5}\\tacc: {:.5}'.format(epoch+1, val_loss, running_corrects))\n",
        "\n",
        "    return val_loss, running_corrects"
      ],
      "id": "bffb2919"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EmKhjrZV9nJ"
      },
      "outputs": [],
      "source": [
        "def gen_trainable(model):\n",
        "    layer = str(np.random.choice(20))\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'encoder' in name:\n",
        "            if layer in name:\n",
        "                param.requires_grad_(True)\n",
        "            else:\n",
        "                param.requires_grad_(False)\n",
        "        else:\n",
        "            param.requires_grad_(True)"
      ],
      "id": "7EmKhjrZV9nJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c5121c",
        "outputId": "d1cf0fae-83a3-4e35-8a84-382d4dcdcb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize network...\n"
          ]
        }
      ],
      "source": [
        "print(\"Initialize network...\")\n",
        "net = SiameseNet('vits').to(Config.device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=Config.learning_rate)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=10)"
      ],
      "id": "f3c5121c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd52afa7",
        "outputId": "241019c3-6fd3-485c-d7d9-c7ddaeba28cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "[1], \ttrain loss: 0.010175\tacc: 0.62057\n",
            "[1], \tval loss: 0.0096074\tacc: 0.70608\n",
            "saving...\n",
            "saving...\n",
            "[2], \ttrain loss: 0.0091058\tacc: 0.6858\n",
            "[2], \tval loss: 0.0089197\tacc: 0.70946\n",
            "saving...\n",
            "saving...\n",
            "[3], \ttrain loss: 0.0083708\tacc: 0.72531\n",
            "[3], \tval loss: 0.0091719\tacc: 0.70946\n",
            "[4], \ttrain loss: 0.0078724\tacc: 0.74608\n",
            "[4], \tval loss: 0.0086805\tacc: 0.73649\n",
            "saving...\n",
            "saving...\n",
            "[5], \ttrain loss: 0.0077151\tacc: 0.76192\n",
            "[5], \tval loss: 0.0085651\tacc: 0.74831\n",
            "saving...\n",
            "saving...\n",
            "[6], \ttrain loss: 0.0074115\tacc: 0.76907\n",
            "[6], \tval loss: 0.0086026\tacc: 0.74155\n",
            "[7], \ttrain loss: 0.0075149\tacc: 0.76345\n",
            "[7], \tval loss: 0.0079966\tacc: 0.76182\n",
            "saving...\n",
            "saving...\n",
            "[8], \ttrain loss: 0.0072673\tacc: 0.77861\n",
            "[8], \tval loss: 0.0076663\tacc: 0.80068\n",
            "saving...\n",
            "saving...\n",
            "[9], \ttrain loss: 0.0070035\tacc: 0.78627\n",
            "[9], \tval loss: 0.0075937\tacc: 0.77703\n",
            "saving...\n",
            "[10], \ttrain loss: 0.0068324\tacc: 0.78968\n",
            "[10], \tval loss: 0.0086859\tacc: 0.73142\n",
            "[11], \ttrain loss: 0.0066079\tacc: 0.80228\n",
            "[11], \tval loss: 0.007315\tacc: 0.78547\n",
            "saving...\n",
            "[12], \ttrain loss: 0.0067803\tacc: 0.7924\n",
            "[12], \tval loss: 0.0076035\tacc: 0.78716\n",
            "[13], \ttrain loss: 0.006568\tacc: 0.80722\n",
            "[13], \tval loss: 0.0078636\tacc: 0.76182\n",
            "[14], \ttrain loss: 0.0066282\tacc: 0.79973\n",
            "[14], \tval loss: 0.0072803\tacc: 0.79561\n",
            "saving...\n",
            "[15], \ttrain loss: 0.0064634\tacc: 0.80926\n",
            "[15], \tval loss: 0.0075941\tacc: 0.78378\n",
            "[16], \ttrain loss: 0.0063036\tacc: 0.81097\n",
            "[16], \tval loss: 0.0078945\tacc: 0.74155\n",
            "[17], \ttrain loss: 0.0061738\tacc: 0.81676\n",
            "[17], \tval loss: 0.0075038\tacc: 0.79561\n",
            "[18], \ttrain loss: 0.0061936\tacc: 0.81182\n",
            "[18], \tval loss: 0.0075663\tacc: 0.80405\n",
            "saving...\n",
            "[19], \ttrain loss: 0.0062787\tacc: 0.81012\n",
            "[19], \tval loss: 0.0073848\tacc: 0.78041\n",
            "[20], \ttrain loss: 0.0062584\tacc: 0.8125\n",
            "[20], \tval loss: 0.0079917\tacc: 0.77027\n",
            "[21], \ttrain loss: 0.0061727\tacc: 0.81778\n",
            "[21], \tval loss: 0.0082846\tacc: 0.74324\n",
            "[22], \ttrain loss: 0.0061891\tacc: 0.8171\n",
            "[22], \tval loss: 0.0088408\tacc: 0.76182\n",
            "[23], \ttrain loss: 0.0058311\tacc: 0.82885\n",
            "[23], \tval loss: 0.0072205\tacc: 0.78547\n",
            "saving...\n",
            "[24], \ttrain loss: 0.0059996\tacc: 0.82272\n",
            "[24], \tval loss: 0.00886\tacc: 0.77196\n",
            "[25], \ttrain loss: 0.0059016\tacc: 0.83294\n",
            "[25], \tval loss: 0.0077861\tacc: 0.77872\n",
            "[26], \ttrain loss: 0.0057587\tacc: 0.83055\n",
            "[26], \tval loss: 0.0083916\tacc: 0.77365\n",
            "[27], \ttrain loss: 0.0055963\tacc: 0.83975\n",
            "[27], \tval loss: 0.0079888\tacc: 0.77534\n",
            "[28], \ttrain loss: 0.0055477\tacc: 0.83839\n",
            "[28], \tval loss: 0.0074083\tacc: 0.80574\n",
            "saving...\n",
            "[29], \ttrain loss: 0.005312\tacc: 0.84877\n",
            "[29], \tval loss: 0.0085242\tacc: 0.77703\n",
            "[30], \ttrain loss: 0.0054975\tacc: 0.84315\n",
            "[30], \tval loss: 0.0082498\tacc: 0.79223\n",
            "[31], \ttrain loss: 0.0055715\tacc: 0.83498\n",
            "[31], \tval loss: 0.0074185\tacc: 0.79392\n",
            "[32], \ttrain loss: 0.0057376\tacc: 0.83617\n",
            "[32], \tval loss: 0.0078959\tacc: 0.7652\n",
            "[33], \ttrain loss: 0.00546\tacc: 0.84281\n",
            "[33], \tval loss: 0.0086675\tacc: 0.78716\n",
            "[34], \ttrain loss: 0.005174\tacc: 0.85303\n",
            "[34], \tval loss: 0.0086961\tacc: 0.77365\n",
            "[35], \ttrain loss: 0.0047492\tacc: 0.8704\n",
            "[35], \tval loss: 0.0089409\tacc: 0.77534\n",
            "[36], \ttrain loss: 0.0048226\tacc: 0.86427\n",
            "[36], \tval loss: 0.0087813\tacc: 0.77534\n",
            "[37], \ttrain loss: 0.0049052\tacc: 0.86018\n",
            "[37], \tval loss: 0.009062\tacc: 0.77027\n",
            "[38], \ttrain loss: 0.0048323\tacc: 0.86427\n",
            "[38], \tval loss: 0.0082694\tacc: 0.78547\n",
            "[39], \ttrain loss: 0.0049134\tacc: 0.85814\n",
            "[39], \tval loss: 0.0087085\tacc: 0.77027\n",
            "[40], \ttrain loss: 0.0047753\tacc: 0.86666\n",
            "[40], \tval loss: 0.0084185\tacc: 0.77027\n",
            "[41], \ttrain loss: 0.0047933\tacc: 0.87006\n",
            "[41], \tval loss: 0.009259\tacc: 0.75845\n",
            "[42], \ttrain loss: 0.004842\tacc: 0.86138\n",
            "[42], \tval loss: 0.0078479\tacc: 0.79392\n",
            "[43], \ttrain loss: 0.0047326\tacc: 0.86751\n",
            "[43], \tval loss: 0.0079797\tacc: 0.78885\n",
            "[44], \ttrain loss: 0.0047282\tacc: 0.87125\n",
            "[44], \tval loss: 0.0080716\tacc: 0.77872\n",
            "[45], \ttrain loss: 0.0046098\tacc: 0.875\n",
            "[45], \tval loss: 0.0091466\tacc: 0.77365\n",
            "[46], \ttrain loss: 0.0047516\tacc: 0.86802\n",
            "[46], \tval loss: 0.0089521\tacc: 0.79223\n",
            "[47], \ttrain loss: 0.0044361\tacc: 0.87926\n",
            "[47], \tval loss: 0.0086269\tacc: 0.78547\n",
            "[48], \ttrain loss: 0.0047364\tacc: 0.86683\n",
            "[48], \tval loss: 0.0087447\tacc: 0.76689\n",
            "[49], \ttrain loss: 0.0046536\tacc: 0.86989\n",
            "[49], \tval loss: 0.0088935\tacc: 0.77534\n",
            "[50], \ttrain loss: 0.0046139\tacc: 0.8721\n",
            "[50], \tval loss: 0.008122\tacc: 0.79054\n",
            "[51], \ttrain loss: 0.0047548\tacc: 0.86683\n",
            "[51], \tval loss: 0.0089226\tacc: 0.77365\n",
            "[52], \ttrain loss: 0.0048374\tacc: 0.86785\n",
            "[52], \tval loss: 0.0090582\tacc: 0.77365\n",
            "[53], \ttrain loss: 0.0047137\tacc: 0.86989\n",
            "[53], \tval loss: 0.0095472\tacc: 0.76182\n",
            "[54], \ttrain loss: 0.0045659\tacc: 0.87432\n",
            "[54], \tval loss: 0.0095111\tacc: 0.77196\n",
            "[55], \ttrain loss: 0.0046304\tacc: 0.87398\n",
            "[55], \tval loss: 0.010062\tacc: 0.75507\n",
            "[56], \ttrain loss: 0.004564\tacc: 0.87687\n",
            "[56], \tval loss: 0.0085981\tacc: 0.79223\n",
            "[57], \ttrain loss: 0.0047248\tacc: 0.8658\n",
            "[57], \tval loss: 0.0088014\tacc: 0.78547\n",
            "[58], \ttrain loss: 0.0047391\tacc: 0.87023\n",
            "[58], \tval loss: 0.0079901\tacc: 0.79899\n",
            "[59], \ttrain loss: 0.0044418\tacc: 0.8876\n",
            "[59], \tval loss: 0.0080631\tacc: 0.78547\n",
            "[60], \ttrain loss: 0.0047702\tacc: 0.86683\n",
            "[60], \tval loss: 0.0091392\tacc: 0.77872\n",
            "[61], \ttrain loss: 0.0046255\tacc: 0.86938\n",
            "[61], \tval loss: 0.0082494\tacc: 0.79223\n",
            "[62], \ttrain loss: 0.0045917\tacc: 0.87381\n",
            "[62], \tval loss: 0.0080366\tacc: 0.78378\n",
            "[63], \ttrain loss: 0.0045677\tacc: 0.87738\n",
            "[63], \tval loss: 0.0089739\tacc: 0.76689\n",
            "[64], \ttrain loss: 0.0047945\tacc: 0.86563\n",
            "[64], \tval loss: 0.0088583\tacc: 0.7973\n",
            "[65], \ttrain loss: 0.0046251\tacc: 0.8721\n",
            "[65], \tval loss: 0.0093364\tacc: 0.77365\n",
            "[66], \ttrain loss: 0.0048882\tacc: 0.86495\n",
            "[66], \tval loss: 0.0088499\tacc: 0.77703\n",
            "[67], \ttrain loss: 0.0047081\tacc: 0.86836\n",
            "[67], \tval loss: 0.0085544\tacc: 0.77703\n",
            "[68], \ttrain loss: 0.0046998\tacc: 0.87381\n",
            "[68], \tval loss: 0.0087306\tacc: 0.78209\n",
            "[69], \ttrain loss: 0.0046396\tacc: 0.86955\n",
            "[69], \tval loss: 0.0085277\tacc: 0.79392\n",
            "[70], \ttrain loss: 0.0045422\tacc: 0.87602\n",
            "[70], \tval loss: 0.0078523\tacc: 0.79223\n",
            "[71], \ttrain loss: 0.0048119\tacc: 0.87006\n",
            "[71], \tval loss: 0.0082919\tacc: 0.80574\n",
            "[72], \ttrain loss: 0.0045431\tacc: 0.87875\n",
            "[72], \tval loss: 0.009214\tacc: 0.78209\n",
            "[73], \ttrain loss: 0.0045837\tacc: 0.87534\n",
            "[73], \tval loss: 0.0089489\tacc: 0.77027\n",
            "[74], \ttrain loss: 0.0045703\tacc: 0.87772\n",
            "[74], \tval loss: 0.0097382\tacc: 0.75507\n",
            "[75], \ttrain loss: 0.0046822\tacc: 0.86904\n",
            "[75], \tval loss: 0.0084832\tacc: 0.78378\n",
            "[76], \ttrain loss: 0.0046862\tacc: 0.87602\n",
            "[76], \tval loss: 0.0081663\tacc: 0.78547\n",
            "[77], \ttrain loss: 0.0045685\tacc: 0.87057\n",
            "[77], \tval loss: 0.0087102\tacc: 0.79223\n",
            "[78], \ttrain loss: 0.0044675\tacc: 0.87449\n",
            "[78], \tval loss: 0.0093346\tacc: 0.76858\n",
            "[79], \ttrain loss: 0.0045567\tacc: 0.87415\n",
            "[79], \tval loss: 0.0084037\tacc: 0.79054\n",
            "[80], \ttrain loss: 0.0046298\tacc: 0.87006\n",
            "[80], \tval loss: 0.0087395\tacc: 0.78547\n",
            "[81], \ttrain loss: 0.0047924\tacc: 0.86461\n",
            "[81], \tval loss: 0.008692\tacc: 0.76858\n",
            "[82], \ttrain loss: 0.0047011\tacc: 0.87517\n",
            "[82], \tval loss: 0.0081473\tacc: 0.79054\n",
            "[83], \ttrain loss: 0.0045455\tacc: 0.87381\n",
            "[83], \tval loss: 0.0084294\tacc: 0.78041\n",
            "[84], \ttrain loss: 0.004695\tacc: 0.87262\n",
            "[84], \tval loss: 0.008211\tacc: 0.79561\n",
            "[85], \ttrain loss: 0.0045823\tacc: 0.87619\n",
            "[85], \tval loss: 0.0086672\tacc: 0.77872\n",
            "[86], \ttrain loss: 0.0044185\tacc: 0.875\n",
            "[86], \tval loss: 0.0085264\tacc: 0.78885\n",
            "[87], \ttrain loss: 0.0046994\tacc: 0.8704\n",
            "[87], \tval loss: 0.0095635\tacc: 0.77534\n",
            "[88], \ttrain loss: 0.0045704\tacc: 0.87534\n",
            "[88], \tval loss: 0.0076046\tacc: 0.80743\n",
            "saving...\n",
            "[89], \ttrain loss: 0.0046152\tacc: 0.87176\n",
            "[89], \tval loss: 0.0086412\tacc: 0.77365\n",
            "[90], \ttrain loss: 0.0046166\tacc: 0.87245\n",
            "[90], \tval loss: 0.0089615\tacc: 0.76689\n",
            "[91], \ttrain loss: 0.0046903\tacc: 0.8704\n",
            "[91], \tval loss: 0.0085844\tacc: 0.79392\n",
            "[92], \ttrain loss: 0.0046037\tacc: 0.87449\n",
            "[92], \tval loss: 0.0091363\tacc: 0.77872\n",
            "[93], \ttrain loss: 0.0049193\tacc: 0.86172\n",
            "[93], \tval loss: 0.0085305\tacc: 0.77703\n",
            "[94], \ttrain loss: 0.0046349\tacc: 0.87193\n",
            "[94], \tval loss: 0.0087128\tacc: 0.74831\n",
            "[95], \ttrain loss: 0.0047939\tacc: 0.867\n",
            "[95], \tval loss: 0.0087841\tacc: 0.77534\n",
            "[96], \ttrain loss: 0.0045995\tacc: 0.87636\n",
            "[96], \tval loss: 0.008436\tacc: 0.7973\n",
            "[97], \ttrain loss: 0.0046722\tacc: 0.87193\n",
            "[97], \tval loss: 0.0084726\tacc: 0.79054\n",
            "[98], \ttrain loss: 0.0046751\tacc: 0.86921\n",
            "[98], \tval loss: 0.0089784\tacc: 0.78716\n",
            "[99], \ttrain loss: 0.0047408\tacc: 0.8704\n",
            "[99], \tval loss: 0.0088775\tacc: 0.77534\n",
            "[100], \ttrain loss: 0.0046134\tacc: 0.87176\n",
            "[100], \tval loss: 0.0091621\tacc: 0.76858\n"
          ]
        }
      ],
      "source": [
        "print(\"Start training...\")\n",
        "\n",
        "best_val_loss = 1000\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "history = []\n",
        "accuracy = []\n",
        "for epoch in range(Config.number_of_epochs):\n",
        "    if epoch%10==0:\n",
        "        gen_trainable(net)\n",
        "    train_loss, train_acc = train(net, criterion, optimizer)\n",
        "    val_loss, val_acc = validate(net, criterion, optimizer)\n",
        "    history.append((train_loss, val_loss))\n",
        "    accuracy.append((train_acc,val_acc))\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(net.state_dict(), './checkpoints/best_loss8.pth')\n",
        "        print('saving...')\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), './checkpoints/best_acc8.pth')\n",
        "        print('saving...')"
      ],
      "id": "dd52afa7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XgaTmmMJoq"
      },
      "source": [
        "### Submission"
      ],
      "id": "D7XgaTmmMJoq"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qgaIk8upMUnv"
      },
      "outputs": [],
      "source": [
        "class FamilyTestDataset(Dataset):\n",
        "    def __init__(self, relations, data_dir, transform, transform2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            relations (string): Data frame with the image paths.\n",
        "            data_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.relations = relations\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.transform2 = transform2\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.relations)\n",
        "\n",
        "    def __getpair__(self, idx):\n",
        "        pair = (\n",
        "            os.path.join(self.data_dir, self.relations.iloc[idx, 0].split(\"-\")[0]),\n",
        "            os.path.join(self.data_dir, self.relations.iloc[idx, 0].split(\"-\")[1]),\n",
        "        )\n",
        "        return pair\n",
        "\n",
        "    def __getlabel__(self, idx) -> int:\n",
        "        return self.relations.iloc[idx, 1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.__getpair__(idx)\n",
        "\n",
        "        im1 = Image.open(pair[0])\n",
        "        im2 = Image.open(pair[1])\n",
        "\n",
        "        img1 = self.transform(im1)\n",
        "        img2 = self.transform(im2)\n",
        "        img3 = self.transform2(im1)\n",
        "        img4 = self.transform2(im2)\n",
        "\n",
        "        return idx, img1, img2, img3, img4\n",
        "def create_test_dataloader(test_image_dir: str, test_relationship_file: str):\n",
        "    df = pd.read_csv(test_relationship_file)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(112),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0., 0., 0.],\n",
        "                             std=[1/255., 1/255., 1/255.])\n",
        "    ])\n",
        "    transform2 = transforms.Compose([\n",
        "        transforms.Resize(160),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                             std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    test_dataset = FamilyTestDataset(\n",
        "        relations=df, data_dir=test_image_dir, transform=transform, transform2=transform2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=200,\n",
        "    )\n",
        "\n",
        "    return test_loader"
      ],
      "id": "qgaIk8upMUnv"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hMW0VRsuO36",
        "outputId": "847a1cd4-9247-44c6-abc1-7c2e96b55aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5310\n"
          ]
        }
      ],
      "source": [
        "def load_classifier(path_to_model_weights: str):\n",
        "    model = SiameseNet('vits')\n",
        "    model.load_state_dict(torch.load(path_to_model_weights))\n",
        "    return model\n",
        "\n",
        "def create_submission(path_to_template: str, path_to_save: str, predictions):\n",
        "    template = pd.read_csv(path_to_template)\n",
        "\n",
        "    # Remember to save as floats as metric is AUC\n",
        "    for row, pred in predictions.items():\n",
        "        template.loc[row, \"is_related\"] = float(pred)\n",
        "\n",
        "    template.to_csv(path_or_buf=path_to_save, index=False)\n",
        "    return\n",
        "\n",
        "\n",
        "def test_classifier(classifier, test_loader):\n",
        "    predictions = {}\n",
        "\n",
        "    classifier.to(Config.device)\n",
        "    classifier.eval()\n",
        "    for i, data in enumerate(test_loader):\n",
        "        row, img1, img2, img3, img4 = data\n",
        "        row, img1, img2, img3, img4 = row.to(Config.device), img1.to(Config.device), img2.to(Config.device), img3.to(Config.device), img4.to(Config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = classifier(img1, img2, img3, img4)\n",
        "\n",
        "        for j in range(len(row)):\n",
        "            predictions[row[j].item()] = output[j].item()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_to_model_weights = \"./checkpoints/best_loss8.pth\"\n",
        "    path_to_template = \"./data/submissions/sample_submission.csv\"\n",
        "    path_to_save = \"./data/submissions/loss8.csv\"\n",
        "\n",
        "    classifier = load_classifier(path_to_model_weights)\n",
        "\n",
        "    test_loader = create_test_dataloader(\n",
        "        \"/content/data/test\",\n",
        "        \"/content/data/submissions/sample_submission.csv\"\n",
        "    )\n",
        "\n",
        "    predictions = test_classifier(\n",
        "        classifier=classifier, test_loader=test_loader\n",
        "    )\n",
        "    print(len(predictions))\n",
        "\n",
        "    create_submission(\n",
        "        path_to_template=path_to_template,\n",
        "        path_to_save=path_to_save,\n",
        "        predictions=predictions,\n",
        "    )"
      ],
      "id": "4hMW0VRsuO36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gbh-JcwMmvu"
      },
      "outputs": [],
      "source": [],
      "id": "_Gbh-JcwMmvu"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6501.415952,
      "end_time": "2022-08-28T08:38:34.596256",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-08-28T06:50:13.180304",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}