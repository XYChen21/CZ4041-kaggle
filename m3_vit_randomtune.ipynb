{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "554OGqJ7OHRJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6u-ARLg-EGUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d387524-49c4-441b-efb3-5d9b01a0fe59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install facenet_pytorch\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "root_dir = \"/content/drive/MyDrive\" # Set appropriate directory\n",
        "os.chdir(root_dir)"
      ],
      "metadata": {
        "id": "nxOIUWa4K6yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce7cc7d-60f6-4f03-aff5-581f9c373eac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/data.zip -d /content"
      ],
      "metadata": {
        "id": "Kld6c1uQNiVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc42f7b7-4708-4ae1-ecb0-aefe245e5197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/data.zip\n",
            "replace /content/__MACOSX/._data? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "import torchvision\n",
        "from random import choice\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# from torch.nn import Parameter\n",
        "# from facenet_pytorch import InceptionResnetV1"
      ],
      "metadata": {
        "id": "-ZNEAPsTNilv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "XcUYWp8ePHHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_file_path = \"./data/train-relationships/train_relationships.csv\"\n",
        "    train_folders_path = \"/content/data/train/\"\n",
        "    val_famillies = \"F09\"\n",
        "    test_relationship_file = \"/content/data/submissions/sample_submission.csv\"\n",
        "    batch_size = 64\n",
        "    number_of_epochs = 100\n",
        "\n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    MIN_NUM_PATCHES = 16\n",
        "    pretrained_vits = '/content/drive/MyDrive/face_transformer/Backbone_VITs_Epoch_2_Batch_12000_Time_2021-03-17-04-05_checkpoint.pth'\n",
        "    pretrained_vit = '/content/drive/MyDrive/face_transformer/Backbone_VIT_Epoch_2_Batch_20000_Time_2021-01-12-16-48_checkpoint.pth'"
      ],
      "metadata": {
        "id": "EZV8Lo3OPG4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "SmeusIO0OKlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KinDataset(Dataset):\n",
        "    def __init__(self, relations, person_to_images_map, transform=None):\n",
        "        self.relations = relations\n",
        "        self.transform = transform\n",
        "        self.person_to_images_map = person_to_images_map\n",
        "        self.ppl = list(person_to_images_map.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.relations)*2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if idx%2==0: #Positive samples\n",
        "            p1, p2 = self.relations[idx//2]\n",
        "            label = 1\n",
        "        else:          #TODO: better way to sample Negative samples\n",
        "            while True:\n",
        "                p1 = choice(self.ppl)\n",
        "                p2 = choice(self.ppl)\n",
        "                if p1 != p2 and (p1, p2) not in self.relations and (p2, p1) not in self.relations:\n",
        "                    break\n",
        "            label = 0\n",
        "\n",
        "        path1, path2 = choice(self.person_to_images_map[p1]), choice(self.person_to_images_map[p2])\n",
        "        img1, img2 = Image.open(path1), Image.open(path2)\n",
        "\n",
        "        if self.transform:\n",
        "            img1, img2 = self.transform(img1), self.transform(img2)\n",
        "\n",
        "        return img1, img2, label"
      ],
      "metadata": {
        "id": "Q0bbbiECNs9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prepare data...\")\n",
        "all_images = glob(Config.train_folders_path + \"*/*/*.jpg\")\n",
        "\n",
        "train_images = [x for x in all_images if Config.val_famillies not in x]\n",
        "val_images = [x for x in all_images if Config.val_famillies in x]\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
        "\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "relationships = pd.read_csv(Config.train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
        "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
        "\n",
        "train_relations = [x for x in relationships if Config.val_famillies not in x[0]]\n",
        "val_relations  = [x for x in relationships if Config.val_famillies in x[0]]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(130),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0., 0., 0.], [1/255., 1/255., 1/255.]),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(130),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0., 0., 0.], [1/255., 1/255., 1/255.]),\n",
        "])\n",
        "\n",
        "trainset = KinDataset(train_relations, train_person_to_images_map, train_transform)\n",
        "valset = KinDataset(val_relations, val_person_to_images_map, val_transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=Config.batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=Config.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "sOwU6jy0OOER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "EpgAjS6yQOVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "        #embed()\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, mask_value)\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n",
        "            ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask = mask)\n",
        "            #embed()\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT_face(nn.Module):\n",
        "    def __init__(self, *, loss_type, GPU_ID, num_class, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.loss_type = loss_type\n",
        "        self.GPU_ID = GPU_ID\n",
        "        if self.loss_type == 'None':\n",
        "            print(\"no loss for vit_face\")\n",
        "        else:\n",
        "            self.loss = CosFace(in_features=dim, out_features=num_class, device_id=self.GPU_ID)\n",
        "\n",
        "    def forward(self, img, label= None , mask = None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        last_hidden_state = x.detach()\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        emb = self.mlp_head(x)\n",
        "        if label is not None:\n",
        "            x = self.loss(emb, label)\n",
        "            return x, emb\n",
        "        else:\n",
        "            return emb, last_hidden_state\n",
        "\n",
        "class ViTs_face(nn.Module):\n",
        "    def __init__(self, *, loss_type, GPU_ID, num_class, image_size, patch_size, ac_patch_size,\n",
        "                         pad, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * ac_patch_size ** 2\n",
        "        assert num_patches > Config.MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.soft_split = nn.Unfold(kernel_size=(ac_patch_size, ac_patch_size), stride=(self.patch_size, self.patch_size), padding=(pad, pad))\n",
        "\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "        self.loss_type = loss_type\n",
        "        self.GPU_ID = GPU_ID\n",
        "        if self.loss_type == 'None':\n",
        "            print(\"no loss for vit_face\")\n",
        "\n",
        "    def forward(self, img, label= None , mask = None, return_lhs=False):\n",
        "        p = self.patch_size\n",
        "        x = self.soft_split(img)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.patch_to_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        last_hidden_state = x.detach()\n",
        "        # print('transformer_out', x.shape)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        emb = self.mlp_head(x)\n",
        "        if label is not None:\n",
        "            x = self.loss(emb, label)\n",
        "            return x, emb\n",
        "        elif return_lhs:\n",
        "            return emb, last_hidden_state\n",
        "        else:\n",
        "            return emb\n",
        "\n",
        "def build_encoder(name):\n",
        "    if name == 'vits':\n",
        "        model = ViTs_face(\n",
        "            loss_type=None,\n",
        "            GPU_ID=Config.device,\n",
        "            num_class=93431,\n",
        "            image_size=112,\n",
        "            patch_size=8,\n",
        "            ac_patch_size=12,\n",
        "            pad=4,\n",
        "            dim=512,\n",
        "            depth=20,\n",
        "            heads=8,\n",
        "            mlp_dim=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "        model.load_state_dict(torch.load(Config.pretrained_vits, map_location=Config.device), strict=False)\n",
        "    else:\n",
        "        model = ViT_face(\n",
        "            image_size=112,\n",
        "            patch_size=8,\n",
        "            loss_type=None,\n",
        "            GPU_ID=Config.device,\n",
        "            num_class=93431,\n",
        "            dim=512,\n",
        "            depth=20,\n",
        "            heads=8,\n",
        "            mlp_dim=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "        model.load_state_dict(torch.load(Config.pretrained_vit, map_location=Config.device), strict=False)\n",
        "    return model"
      ],
      "metadata": {
        "id": "J2BmRy-tQAI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNet(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = build_encoder(name)\n",
        "        self.embed_size = 512\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.embed_size*3, self.embed_size*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(self.embed_size*4, self.embed_size*1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(self.embed_size*1, self.embed_size//4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Linear(self.embed_size//4+1,self.embed_size//16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(self.embed_size//16, 1),\n",
        "        )\n",
        "        self.encoder.requires_grad_(False)\n",
        "\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        x1 = self.encoder(input1)\n",
        "        x2 = self.encoder(input2)\n",
        "        x3 = x1-x2\n",
        "        x5 = torch.pow(x1,2)\n",
        "        x6 = torch.pow(x2,2)\n",
        "        x = torch.cat([x3,x5+x6,x1*x2],dim=-1)\n",
        "        x = self.fc(x)\n",
        "        cos_dis=1-F.cosine_similarity(x1,x2,dim=-1)\n",
        "        x = torch.cat([x,cos_dis.unsqueeze(1)],dim=-1)\n",
        "        result = self.last(x)\n",
        "        result=torch.sigmoid(result)\n",
        "        return result"
      ],
      "metadata": {
        "id": "veFOlXh5Ro-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "wua8Ns_yZ99j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, criterion, optimizer):\n",
        "    net.train()\n",
        "    train_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img1, img2, label = batch\n",
        "        img1, img2, label = img1.to(Config.device), img2.to(Config.device), label.float().view(-1,1).to(Config.device)\n",
        "        output = net(img1, img2)\n",
        "        preds = output>0.5\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == (label>0.5))\n",
        "\n",
        "    train_loss /= len(trainset)\n",
        "    running_corrects = running_corrects.item()/len(trainset)\n",
        "    print('[{}], \\ttrain loss: {:.5}\\tacc: {:.5}'.format(epoch+1, train_loss, running_corrects))\n",
        "    return train_loss, running_corrects"
      ],
      "metadata": {
        "id": "fTjXq8rhZzZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, criterion, optimizer):\n",
        "    net.eval()\n",
        "    val_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for batch in valloader:\n",
        "        img1, img2, label = batch\n",
        "        img1, img2, label = img1.to(Config.device), img2.to(Config.device), label.float().view(-1,1).to(Config.device)\n",
        "        with torch.no_grad():\n",
        "            output = net(img1, img2)\n",
        "            preds = output>0.5\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == (label>0.5))\n",
        "\n",
        "    val_loss /= len(valset)\n",
        "    running_corrects = running_corrects.item()/len(valset)\n",
        "    print('[{}], \\tval loss: {:.5}\\tacc: {:.5}'.format(epoch+1, val_loss, running_corrects))\n",
        "\n",
        "    return val_loss, running_corrects"
      ],
      "metadata": {
        "id": "1apQ1JWOaWFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_trainable(model):\n",
        "    layer = str(np.random.choice(20))\n",
        "    trainable_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'encoder' in name:\n",
        "            if layer in name:\n",
        "                param.requires_grad_(True)\n",
        "                trainable_params.append(param)\n",
        "            else:\n",
        "                param.requires_grad_(False)\n",
        "        else:\n",
        "            param.requires_grad_(True)\n",
        "            trainable_params.append(param)\n",
        "    return trainable_params"
      ],
      "metadata": {
        "id": "Y3oI2m0lvgFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initialize network...\")\n",
        "net = SiameseNet('vits').to(Config.device)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "avPivYV5anem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training...\")\n",
        "\n",
        "best_val_loss = 1000\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "history = []\n",
        "accuracy = []\n",
        "for epoch in range(Config.number_of_epochs):\n",
        "    if epoch%5==0:\n",
        "        optimizer = torch.optim.Adam(gen_trainable(net), lr=Config.learning_rate)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, patience=10)\n",
        "    train_loss, train_acc = train(net, criterion, optimizer)\n",
        "    val_loss, val_acc = validate(net, criterion, optimizer)\n",
        "    history.append((train_loss, val_loss))\n",
        "    accuracy.append((train_acc,val_acc))\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(net.state_dict(), './checkpoints/best_loss11.pth')\n",
        "        print('saving...')\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), './checkpoints/best_acc11.pth')\n",
        "        print('saving...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13bK8Ad7a45k",
        "outputId": "9737bec1-43b0-49da-e7a7-20dddba08de8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "[1], \ttrain loss: 0.010304\tacc: 0.59401\n",
            "[1], \tval loss: 0.0096748\tacc: 0.69595\n",
            "saving...\n",
            "saving...\n",
            "[2], \ttrain loss: 0.0090074\tacc: 0.69567\n",
            "[2], \tval loss: 0.0084115\tacc: 0.75676\n",
            "saving...\n",
            "saving...\n",
            "[3], \ttrain loss: 0.0082318\tacc: 0.74319\n",
            "[3], \tval loss: 0.0080198\tacc: 0.77703\n",
            "saving...\n",
            "saving...\n",
            "[4], \ttrain loss: 0.0077151\tacc: 0.76345\n",
            "[4], \tval loss: 0.0073513\tacc: 0.78378\n",
            "saving...\n",
            "saving...\n",
            "[5], \ttrain loss: 0.0072793\tacc: 0.78134\n",
            "[5], \tval loss: 0.0066549\tacc: 0.81926\n",
            "saving...\n",
            "saving...\n",
            "[6], \ttrain loss: 0.0067698\tacc: 0.79956\n",
            "[6], \tval loss: 0.0064359\tacc: 0.80405\n",
            "saving...\n",
            "[7], \ttrain loss: 0.0067533\tacc: 0.80279\n",
            "[7], \tval loss: 0.0084403\tacc: 0.75845\n",
            "[8], \ttrain loss: 0.0065476\tacc: 0.80943\n",
            "[8], \tval loss: 0.0069252\tacc: 0.80405\n",
            "[9], \ttrain loss: 0.0060266\tacc: 0.82902\n",
            "[9], \tval loss: 0.0076187\tacc: 0.79054\n",
            "[10], \ttrain loss: 0.0062416\tacc: 0.82084\n",
            "[10], \tval loss: 0.0072863\tacc: 0.80574\n",
            "[11], \ttrain loss: 0.0056893\tacc: 0.84537\n",
            "[11], \tval loss: 0.0066888\tacc: 0.80405\n",
            "[12], \ttrain loss: 0.0054143\tacc: 0.85286\n",
            "[12], \tval loss: 0.007464\tacc: 0.79392\n",
            "[13], \ttrain loss: 0.0055078\tacc: 0.84826\n",
            "[13], \tval loss: 0.0075375\tacc: 0.77872\n",
            "[14], \ttrain loss: 0.0052662\tacc: 0.85371\n",
            "[14], \tval loss: 0.0073441\tacc: 0.78378\n",
            "[15], \ttrain loss: 0.0050857\tacc: 0.86172\n",
            "[15], \tval loss: 0.0059347\tacc: 0.83615\n",
            "saving...\n",
            "saving...\n",
            "[16], \ttrain loss: 0.0050232\tacc: 0.86359\n",
            "[16], \tval loss: 0.0077764\tacc: 0.79223\n",
            "[17], \ttrain loss: 0.0049817\tacc: 0.85916\n",
            "[17], \tval loss: 0.007038\tacc: 0.80405\n",
            "[18], \ttrain loss: 0.0048557\tacc: 0.86819\n",
            "[18], \tval loss: 0.0072283\tacc: 0.82095\n",
            "[19], \ttrain loss: 0.0048717\tacc: 0.86546\n",
            "[19], \tval loss: 0.0083413\tacc: 0.7652\n",
            "[20], \ttrain loss: 0.0044897\tacc: 0.88011\n",
            "[20], \tval loss: 0.0076194\tacc: 0.78547\n",
            "[21], \ttrain loss: 0.0045323\tacc: 0.88232\n",
            "[21], \tval loss: 0.0085412\tacc: 0.77703\n",
            "[22], \ttrain loss: 0.0043293\tacc: 0.88607\n",
            "[22], \tval loss: 0.010739\tacc: 0.77365\n",
            "[23], \ttrain loss: 0.0043788\tacc: 0.88777\n",
            "[23], \tval loss: 0.009135\tacc: 0.76858\n",
            "[24], \ttrain loss: 0.0043753\tacc: 0.88232\n",
            "[24], \tval loss: 0.0075817\tacc: 0.78041\n",
            "[25], \ttrain loss: 0.0043084\tacc: 0.88828\n",
            "[25], \tval loss: 0.0072812\tacc: 0.82264\n",
            "[26], \ttrain loss: 0.0041453\tacc: 0.8951\n",
            "[26], \tval loss: 0.01194\tacc: 0.75507\n",
            "[27], \ttrain loss: 0.004276\tacc: 0.88403\n",
            "[27], \tval loss: 0.0087989\tacc: 0.79223\n",
            "[28], \ttrain loss: 0.0041505\tacc: 0.89135\n",
            "[28], \tval loss: 0.0082715\tacc: 0.78885\n",
            "[29], \ttrain loss: 0.0040438\tacc: 0.89561\n",
            "[29], \tval loss: 0.010072\tacc: 0.78716\n",
            "[30], \ttrain loss: 0.0037777\tacc: 0.90429\n",
            "[30], \tval loss: 0.0079914\tacc: 0.79561\n",
            "[31], \ttrain loss: 0.0037765\tacc: 0.90497\n",
            "[31], \tval loss: 0.011059\tacc: 0.79223\n",
            "[32], \ttrain loss: 0.0036744\tacc: 0.90531\n",
            "[32], \tval loss: 0.011008\tacc: 0.77365\n",
            "[33], \ttrain loss: 0.0038484\tacc: 0.90344\n",
            "[33], \tval loss: 0.013448\tacc: 0.75338\n",
            "[34], \ttrain loss: 0.0036749\tacc: 0.90634\n",
            "[34], \tval loss: 0.010668\tacc: 0.77872\n",
            "[35], \ttrain loss: 0.0036479\tacc: 0.90923\n",
            "[35], \tval loss: 0.013848\tacc: 0.74493\n",
            "[36], \ttrain loss: 0.0034307\tacc: 0.91587\n",
            "[36], \tval loss: 0.010952\tacc: 0.76182\n",
            "[37], \ttrain loss: 0.0035299\tacc: 0.91042\n",
            "[37], \tval loss: 0.010578\tacc: 0.77534\n",
            "[38], \ttrain loss: 0.0035277\tacc: 0.91213\n",
            "[38], \tval loss: 0.008953\tacc: 0.7973\n",
            "[39], \ttrain loss: 0.0035176\tacc: 0.9123\n",
            "[39], \tval loss: 0.010285\tacc: 0.79899\n",
            "[40], \ttrain loss: 0.0033659\tacc: 0.91706\n",
            "[40], \tval loss: 0.013859\tacc: 0.75\n",
            "[41], \ttrain loss: 0.0031354\tacc: 0.91996\n",
            "[41], \tval loss: 0.012013\tacc: 0.73986\n",
            "[42], \ttrain loss: 0.0033914\tacc: 0.91655\n",
            "[42], \tval loss: 0.0089598\tacc: 0.76014\n",
            "[43], \ttrain loss: 0.003327\tacc: 0.91536\n",
            "[43], \tval loss: 0.012147\tacc: 0.75845\n",
            "[44], \ttrain loss: 0.0032751\tacc: 0.91979\n",
            "[44], \tval loss: 0.011752\tacc: 0.73818\n",
            "[45], \ttrain loss: 0.0033199\tacc: 0.91962\n",
            "[45], \tval loss: 0.010258\tacc: 0.76351\n",
            "[46], \ttrain loss: 0.0032693\tacc: 0.91945\n",
            "[46], \tval loss: 0.011656\tacc: 0.78041\n",
            "[47], \ttrain loss: 0.0031521\tacc: 0.92388\n",
            "[47], \tval loss: 0.013491\tacc: 0.76689\n",
            "[48], \ttrain loss: 0.0032029\tacc: 0.91877\n",
            "[48], \tval loss: 0.016771\tacc: 0.74493\n",
            "[49], \ttrain loss: 0.003347\tacc: 0.91723\n",
            "[49], \tval loss: 0.01068\tacc: 0.80068\n",
            "[50], \ttrain loss: 0.002956\tacc: 0.92473\n",
            "[50], \tval loss: 0.010442\tacc: 0.78209\n",
            "[51], \ttrain loss: 0.0076249\tacc: 0.77963\n",
            "[51], \tval loss: 0.0089735\tacc: 0.74831\n",
            "[52], \ttrain loss: 0.0067375\tacc: 0.80399\n",
            "[52], \tval loss: 0.0078198\tacc: 0.76351\n",
            "[53], \ttrain loss: 0.0062067\tacc: 0.82204\n",
            "[53], \tval loss: 0.008187\tacc: 0.74324\n",
            "[54], \ttrain loss: 0.0064686\tacc: 0.81642\n",
            "[54], \tval loss: 0.0076288\tacc: 0.77872\n",
            "[55], \ttrain loss: 0.0064713\tacc: 0.81318\n",
            "[55], \tval loss: 0.0083015\tacc: 0.75338\n",
            "[56], \ttrain loss: 0.0059034\tacc: 0.83345\n",
            "[56], \tval loss: 0.0081843\tacc: 0.75845\n",
            "[57], \ttrain loss: 0.0060078\tacc: 0.83191\n",
            "[57], \tval loss: 0.0077402\tacc: 0.77703\n",
            "[58], \ttrain loss: 0.0060485\tacc: 0.83106\n",
            "[58], \tval loss: 0.0086435\tacc: 0.76182\n",
            "[59], \ttrain loss: 0.0058404\tacc: 0.8343\n",
            "[59], \tval loss: 0.0086778\tacc: 0.74155\n",
            "[60], \ttrain loss: 0.0061567\tacc: 0.82067\n",
            "[60], \tval loss: 0.0081195\tacc: 0.77534\n",
            "[61], \ttrain loss: 0.0052683\tacc: 0.86614\n",
            "[61], \tval loss: 0.0079955\tacc: 0.78041\n",
            "[62], \ttrain loss: 0.0048296\tacc: 0.86938\n",
            "[62], \tval loss: 0.0087066\tacc: 0.77872\n",
            "[63], \ttrain loss: 0.0044693\tacc: 0.88198\n",
            "[63], \tval loss: 0.0077796\tacc: 0.78378\n",
            "[64], \ttrain loss: 0.0042482\tacc: 0.88794\n",
            "[64], \tval loss: 0.00843\tacc: 0.78041\n",
            "[65], \ttrain loss: 0.00413\tacc: 0.89373\n",
            "[65], \tval loss: 0.0091368\tacc: 0.79392\n",
            "[66], \ttrain loss: 0.0040805\tacc: 0.89016\n",
            "[66], \tval loss: 0.0084325\tacc: 0.77872\n",
            "[67], \ttrain loss: 0.0040703\tacc: 0.89646\n",
            "[67], \tval loss: 0.0086929\tacc: 0.76858\n",
            "[68], \ttrain loss: 0.0040562\tacc: 0.89731\n",
            "[68], \tval loss: 0.011041\tacc: 0.73986\n",
            "[69], \ttrain loss: 0.0038868\tacc: 0.90225\n",
            "[69], \tval loss: 0.01098\tacc: 0.76689\n",
            "[70], \ttrain loss: 0.0040573\tacc: 0.89373\n",
            "[70], \tval loss: 0.0099741\tacc: 0.75\n",
            "[71], \ttrain loss: 0.0037\tacc: 0.90838\n",
            "[71], \tval loss: 0.016482\tacc: 0.73311\n",
            "[72], \ttrain loss: 0.0037627\tacc: 0.90548\n",
            "[72], \tval loss: 0.0093662\tacc: 0.78041\n",
            "[73], \ttrain loss: 0.0036685\tacc: 0.90463\n",
            "[73], \tval loss: 0.012343\tacc: 0.73818\n",
            "[74], \ttrain loss: 0.0037507\tacc: 0.89969\n",
            "[74], \tval loss: 0.013305\tacc: 0.72804\n",
            "[75], \ttrain loss: 0.0036459\tacc: 0.9014\n",
            "[75], \tval loss: 0.012549\tacc: 0.72466\n",
            "[76], \ttrain loss: 0.0036484\tacc: 0.90804\n",
            "[76], \tval loss: 0.012222\tacc: 0.74493\n",
            "[77], \ttrain loss: 0.0032544\tacc: 0.9174\n",
            "[77], \tval loss: 0.011579\tacc: 0.75676\n",
            "[78], \ttrain loss: 0.0033321\tacc: 0.91604\n",
            "[78], \tval loss: 0.010534\tacc: 0.7348\n",
            "[79], \ttrain loss: 0.0033432\tacc: 0.91485\n",
            "[79], \tval loss: 0.012478\tacc: 0.73986\n",
            "[80], \ttrain loss: 0.003554\tacc: 0.91127\n",
            "[80], \tval loss: 0.012746\tacc: 0.72804\n",
            "[81], \ttrain loss: 0.0034617\tacc: 0.91485\n",
            "[81], \tval loss: 0.01194\tacc: 0.77027\n",
            "[82], \ttrain loss: 0.0033805\tacc: 0.91894\n",
            "[82], \tval loss: 0.014054\tacc: 0.72973\n",
            "[83], \ttrain loss: 0.0033056\tacc: 0.92013\n",
            "[83], \tval loss: 0.015614\tacc: 0.72128\n",
            "[84], \ttrain loss: 0.0034429\tacc: 0.91638\n",
            "[84], \tval loss: 0.014339\tacc: 0.71622\n",
            "[85], \ttrain loss: 0.0033409\tacc: 0.91655\n",
            "[85], \tval loss: 0.014765\tacc: 0.71453\n",
            "[86], \ttrain loss: 0.0033206\tacc: 0.91979\n",
            "[86], \tval loss: 0.014408\tacc: 0.71284\n",
            "[87], \ttrain loss: 0.0032362\tacc: 0.9186\n",
            "[87], \tval loss: 0.012044\tacc: 0.75845\n",
            "[88], \ttrain loss: 0.0031029\tacc: 0.92268\n",
            "[88], \tval loss: 0.016814\tacc: 0.73142\n",
            "[89], \ttrain loss: 0.0031589\tacc: 0.92268\n",
            "[89], \tval loss: 0.014511\tacc: 0.7348\n",
            "[90], \ttrain loss: 0.0031623\tacc: 0.92064\n",
            "[90], \tval loss: 0.014003\tacc: 0.71622\n",
            "[91], \ttrain loss: 0.0031779\tacc: 0.91945\n",
            "[91], \tval loss: 0.012637\tacc: 0.75169\n",
            "[92], \ttrain loss: 0.002999\tacc: 0.92098\n",
            "[92], \tval loss: 0.015731\tacc: 0.72804\n",
            "[93], \ttrain loss: 0.0030534\tacc: 0.92609\n",
            "[93], \tval loss: 0.01929\tacc: 0.75507\n",
            "[94], \ttrain loss: 0.0029981\tacc: 0.92575\n",
            "[94], \tval loss: 0.016275\tacc: 0.77365\n",
            "[95], \ttrain loss: 0.0029581\tacc: 0.93069\n",
            "[95], \tval loss: 0.021832\tacc: 0.68919\n",
            "[96], \ttrain loss: 0.0028143\tacc: 0.93069\n",
            "[96], \tval loss: 0.017695\tacc: 0.73818\n",
            "[97], \ttrain loss: 0.0028864\tacc: 0.92677\n",
            "[97], \tval loss: 0.021828\tacc: 0.70439\n",
            "[98], \ttrain loss: 0.0028476\tacc: 0.93188\n",
            "[98], \tval loss: 0.016206\tacc: 0.71453\n",
            "[99], \ttrain loss: 0.0029804\tacc: 0.92967\n",
            "[99], \tval loss: 0.013698\tacc: 0.71115\n",
            "[100], \ttrain loss: 0.0029721\tacc: 0.92745\n",
            "[100], \tval loss: 0.020801\tacc: 0.71115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission"
      ],
      "metadata": {
        "id": "eRYlIf6pv66-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FamilyTestDataset(Dataset):\n",
        "    def __init__(self, relations, data_dir, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            relations (string): Data frame with the image paths.\n",
        "            data_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.relations = relations\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.relations)\n",
        "\n",
        "    def __getpair__(self, idx):\n",
        "        pair = (\n",
        "            os.path.join(self.data_dir, self.relations.iloc[idx, 0].split(\"-\")[0]),\n",
        "            os.path.join(self.data_dir, self.relations.iloc[idx, 0].split(\"-\")[1]),\n",
        "        )\n",
        "        return pair\n",
        "\n",
        "    def __getlabel__(self, idx) -> int:\n",
        "        return self.relations.iloc[idx, 1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.__getpair__(idx)\n",
        "\n",
        "        im1 = Image.open(pair[0])\n",
        "        im2 = Image.open(pair[1])\n",
        "\n",
        "        img1 = self.transform(im1)\n",
        "        img2 = self.transform(im2)\n",
        "\n",
        "        return idx, img1, img2\n",
        "def create_test_dataloader(test_image_dir: str, test_relationship_file: str):\n",
        "    df = pd.read_csv(test_relationship_file)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(112),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0., 0., 0.],\n",
        "                             std=[1/255., 1/255., 1/255.])\n",
        "    ])\n",
        "\n",
        "    test_dataset = FamilyTestDataset(\n",
        "        relations=df, data_dir=test_image_dir, transform=transform\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=200,\n",
        "    )\n",
        "\n",
        "    return test_loader"
      ],
      "metadata": {
        "id": "ooJmCeQOcxL8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_classifier(path_to_model_weights: str):\n",
        "    model = SiameseNet('vits')\n",
        "    model.load_state_dict(torch.load(path_to_model_weights))\n",
        "    return model\n",
        "\n",
        "def create_submission(path_to_template: str, path_to_save: str, predictions):\n",
        "    template = pd.read_csv(path_to_template)\n",
        "\n",
        "    # Remember to save as floats as metric is AUC\n",
        "    for row, pred in predictions.items():\n",
        "        template.loc[row, \"is_related\"] = float(pred)\n",
        "\n",
        "    template.to_csv(path_or_buf=path_to_save, index=False)\n",
        "    return\n",
        "\n",
        "\n",
        "def test_classifier(classifier, test_loader):\n",
        "    predictions = {}\n",
        "\n",
        "    classifier.to(Config.device)\n",
        "    classifier.eval()\n",
        "    for i, data in enumerate(test_loader):\n",
        "        row, img1, img2 = data\n",
        "        row, img1, img2 = row.to(Config.device), img1.to(Config.device), img2.to(Config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = classifier(img1, img2)\n",
        "\n",
        "        for j in range(len(row)):\n",
        "            predictions[row[j].item()] = output[j].item()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_to_model_weights = \"./checkpoints/best_acc9.pth\"\n",
        "    path_to_template = \"./data/submissions/sample_submission.csv\"\n",
        "    path_to_save = \"./data/submissions/acc9.csv\"\n",
        "\n",
        "    classifier = load_classifier(path_to_model_weights)\n",
        "\n",
        "    test_loader = create_test_dataloader(\n",
        "        \"/content/data/test\",\n",
        "        \"/content/data/submissions/sample_submission.csv\"\n",
        "    )\n",
        "\n",
        "    predictions = test_classifier(\n",
        "        classifier=classifier, test_loader=test_loader\n",
        "    )\n",
        "    print(len(predictions))\n",
        "\n",
        "    create_submission(\n",
        "        path_to_template=path_to_template,\n",
        "        path_to_save=path_to_save,\n",
        "        predictions=predictions,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YYPLGvgv9GW",
        "outputId": "59bd3176-73eb-48d8-d699-e8381f2800a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkmKB47XID_i"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}